{
  "name": "Image Captioner v3 - Production Ready",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "caption-v3",
        "responseMode": "responseNode",
        "options": {}
      },
      "id": "webhook",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1.1,
      "position": [240, 400],
      "webhookId": "caption-v3-webhook"
    },
    {
      "parameters": {
        "jsCode": "// Setup and validation\nconst body = $input.item.json.body;\nconst images = body.images || [];\n\nif (!images || images.length === 0) {\n  throw new Error('No images provided');\n}\n\n// ============================================\n// LORA TOKEN HANDLING\n// ============================================\nconst loraToken = body.config?.lora_token || body.lora_token || null;\n\n// Validate LoRA token format if provided\nif (loraToken) {\n  if (!/^[a-zA-Z0-9]{3,10}$/.test(loraToken)) {\n    throw new Error(`Invalid LoRA token format: \"${loraToken}\". Should be 3-10 alphanumeric characters (e.g., \"ohwx\", \"sks\", \"zwx\")`);\n  }\n}\n\nconst config = {\n  prompt_template: body.config?.prompt_template || 'general',\n  detail_level: body.config?.detail_level || 'high',\n  enable_refinement: body.config?.enable_refinement !== false,\n  quality_threshold: body.config?.quality_threshold || 0.75,\n  max_caption_length: body.config?.max_caption_length || 150,\n  remove_subject_details: body.config?.remove_subject_details || false,\n  lora_token: loraToken\n};\n\nconst prompts = {\n  general:\n    \"Provide a concise, objective description suitable for training. Describe what is visible: subjects present, key elements, composition/framing, lighting direction/quality/intensity, color palette, materials/textures, and any notable spatial context. Avoid brand names, identities, opinions, or narrative language.\",\n\n  object:\n    \"Describe this image for object LoRA training. Focus on environment and optics: lighting direction/quality/intensity and shadows/reflections, camera–subject relationship (angle, distance, depth of field, framing), background context and support surfaces, and general color/material cues visible without naming or identifying the object specifically. Avoid brand names, unique labels, or subjective adjectives.\",\n\n  character:\n    \"Describe this image for character LoRA training. Emphasize pose and positioning in space, lighting direction/quality/intensity and shadows, camera–subject relationship (angle, distance, focal-length feel, depth of field, framing), and surroundings. Do not list facial identity details or unique identifiers; avoid brand/style words.\",\n\n  style:\n    \"Describe the overall scene succinctly, then the dominant color palette and the quality of light (direction, softness/hardness, contrast, time-of-day feel). Mention composition tendencies or rendering approach if evident (e.g., minimal, high-contrast, soft-gradation). End the caption with: 'in the style of {{STYLE}}'. Do not identify brands or subjects.\",\n\n  scene:\n    \"Describe the setting and visible elements with attention to spatial layout and depth cues, composition/framing lines, lighting direction/quality/intensity, and palette. Include materials/textures of major surfaces. Avoid identities, brands, and subjective adjectives.\",\n\n  edit:\n    \"Write one imperative instruction that specifies only the desired change to the input image. State the target region or element, the change (add/remove/replace/adjust), the magnitude or constraints, and preserve everything else. Example: 'Replace <X> with <Y> in <region>, match perspective and scale, preserve original lighting and colors elsewhere, keep background and composition unchanged.'\"\n};\n\nlet finalPrompt = prompts[config.prompt_template] || prompts.general;\nif (loraToken) {\n  finalPrompt = `IMPORTANT: Begin your description with the trigger token \"${loraToken}\". Then provide the rest of the description.\\n\\n${finalPrompt}`;\n}\n\nreturn {\n  images: images,\n  config: config,\n  prompt: finalPrompt,\n  lora_token: loraToken,\n  job_id: body.job_id || 'job_' + Date.now(),\n  started_at: new Date().toISOString()\n};"
      },
      "id": "setup",
      "name": "Setup",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [460, 400]
    },
    {
      "parameters": {
        "fieldToSplitOut": "images",
        "include": "allOtherFields",
        "options": {}
      },
      "id": "split",
      "name": "Split Out",
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [680, 400]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"gpt-4o\",\n  \"messages\": [{\n    \"role\": \"user\",\n    \"content\": [\n      {\"type\": \"text\", \"text\": \"{{ $('Setup').item.json.prompt }}\"},\n      {\"type\": \"image_url\", \"image_url\": {\"url\": \"{{ $json.images.url }}\", \"detail\": \"{{ $('Setup').item.json.config.detail_level }}\"}}\n    ]\n  }],\n  \"max_tokens\": 300\n}",
        "options": {}
      },
      "id": "caption",
      "name": "Caption",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [900, 400],
      "credentials": {
        "openAiApi": {
          "id": "YOUR_OPENAI_CREDENTIAL_ID",
          "name": "OpenAI API"
        }
      },
      "continueOnFail": true
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "leftValue": "={{ $('Setup').item.json.config.prompt_template }}",
              "rightValue": "object",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            },
            {
              "leftValue": "={{ $('Setup').item.json.config.prompt_template }}",
              "rightValue": "character",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "or"
        }
      },
      "id": "if_subject_removal",
      "name": "Check Subject Removal",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [1120, 400]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a caption editor for LoRA training datasets. Remove subject-specific identifying details while preserving visual descriptions of the scene, lighting, and composition.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"{{ $('Setup').item.json.config.lora_token ? 'CRITICAL: The caption MUST start with \\\"' + $('Setup').item.json.config.lora_token + '\\\". Do not remove this token.\\n\\n' : '' }}Original caption: {{ $('Caption').item.json.choices[0].message.content }}\\n\\nRewrite this caption removing identifying details about the subject.{{ $('Setup').item.json.config.lora_token ? ' Start with \\\"' + $('Setup').item.json.config.lora_token + '\\\" followed by the description.' : '' }}\\n\\nPreserve: pose, lighting, composition, scene elements.\\nRemove: names, identities, brands, specific locations.\"\n    }\n  ],\n  \"max_tokens\": 300\n}",
        "options": {}
      },
      "id": "subject_removal",
      "name": "Remove Subject Details",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [1340, 288],
      "credentials": {
        "openAiApi": {
          "id": "YOUR_OPENAI_CREDENTIAL_ID",
          "name": "OpenAI API"
        }
      },
      "continueOnFail": true
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Process each caption individually\nconst input = $input.item.json;\n\n// Handle API failure\nif (input.error || !input.choices) {\n  return {\n    image_id: $('Split Out').item.json.images.id || 'unknown',\n    filename: $('Split Out').item.json.images.filename || 'unknown',\n    caption: null,\n    error: input.error?.message || 'API call failed',\n    success: false,\n    needs_refinement: false,\n    lora_token_valid: false\n  };\n}\n\n// Extract and score caption\nconst caption = input.choices[0].message.content.trim();\nconst words = caption.split(/\\s+/);\nconst wordCount = words.length;\n\n// Get config safely\nconst setupNode = $('Setup').item.json;\nconst config = setupNode.config || {};\n\n// ============================================\n// LORA TOKEN VALIDATION\n// ============================================\nlet loraTokenValid = true;\nlet loraTokenPosition = null;\nlet loraTokenIssue = null;\n\nconst expectedToken = config.lora_token;\n\nif (expectedToken) {\n  const firstWord = words[0]?.toLowerCase();\n  const tokenMatch = firstWord === expectedToken.toLowerCase();\n  \n  if (!tokenMatch) {\n    loraTokenValid = false;\n    loraTokenIssue = `Expected LoRA token \"${expectedToken}\" at start, but caption starts with \"${words[0] || 'nothing'}\"`;\n  } else {\n    loraTokenPosition = 0;\n  }\n  \n  const tokenCount = caption.toLowerCase().split(expectedToken.toLowerCase()).length - 1;\n  if (tokenCount > 1) {\n    loraTokenIssue = `LoRA token \"${expectedToken}\" appears ${tokenCount} times (should appear once)`;\n  }\n} else {\n  loraTokenValid = true;\n}\n\n// ============================================\n// QUALITY CHECKS\n// ============================================\nconst checks = {\n  hasMinLength: wordCount >= 10,\n  hasMaxLength: wordCount <= 200,\n  hasSubject: /\\b(person|character|figure|animal|object|subject)\\b/i.test(caption),\n  hasVisualDetails: /\\b(color|light|style|composition|texture|material)\\b/i.test(caption),\n  noErrors: !/\\b(error|unable|cannot|sorry)\\b/i.test(caption),\n  notTooVague: !/^(This is|The image shows|I see|Here is)/.test(caption),\n  hasLighting: /\\b(light|shadow|bright|dark|illuminated|backlit|glow)\\b/i.test(caption),\n  hasComposition: /\\b(foreground|background|center|positioned|angle|perspective|frame)\\b/i.test(caption),\n  loraTokenValid: expectedToken ? loraTokenValid : true\n};\n\nconst qualityScore = Object.values(checks).filter(Boolean).length / Object.keys(checks).length;\nconst needsRefinement = config.enable_refinement && qualityScore < (config.quality_threshold || 0.75);\n\nreturn {\n  image_id: $('Split Out').item.json.images.id || 'unknown',\n  filename: $('Split Out').item.json.images.filename || 'unknown',\n  caption: caption,\n  word_count: wordCount,\n  quality_score: Math.round(qualityScore * 100) / 100,\n  quality_checks: checks,\n  needs_refinement: needsRefinement,\n  success: true,\n  config: config,\n  lora_token: expectedToken || null,\n  lora_token_valid: loraTokenValid,\n  lora_token_position: loraTokenPosition,\n  lora_token_issue: loraTokenIssue\n};"
      },
      "id": "score_quality",
      "name": "Score Quality",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1560, 400]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "leftValue": "={{ $json.needs_refinement }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            },
            {
              "leftValue": "={{ $json.success }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "id": "check_refinement",
      "name": "Check Refinement",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [1780, 400]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"Refine image captions to be more detailed and specific for AI training. Focus on concrete, objective visual details.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"{{ $json.config.lora_token ? 'CRITICAL: Keep \\\"' + $json.config.lora_token + '\\\" as the first word.\\n\\n' : '' }}Improve this caption with more concrete visual details about materials, textures, colors, lighting, and composition:\\n\\n{{ $json.caption }}\\n\\n{{ $json.config.lora_token ? 'Remember: \\\"' + $json.config.lora_token + '\\\" must remain the first word.' : '' }}\\n\\nAdd 20-30% more specific visual details.\"\n    }\n  ],\n  \"max_tokens\": 300\n}",
        "options": {}
      },
      "id": "refine",
      "name": "Refine Caption",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [2000, 288],
      "credentials": {
        "openAiApi": {
          "id": "YOUR_OPENAI_CREDENTIAL_ID",
          "name": "OpenAI API"
        }
      },
      "continueOnFail": true
    },
    {
      "parameters": {
        "jsCode": "// Process refinement and re-validate LoRA token\nconst input = $input.item.json;\nconst original = $node['Score Quality'].json;\n\nif (input.error || !input.choices) {\n  return {\n    ...original,\n    was_refined: false,\n    refinement_failed: true\n  };\n}\n\nconst refined = input.choices[0].message.content.trim();\nconst words = refined.split(/\\s+/);\nconst wordCount = words.length;\n\n// Re-validate LoRA token\nlet loraTokenValid = true;\nlet loraTokenIssue = null;\nconst expectedToken = original.lora_token;\n\nif (expectedToken) {\n  const firstWord = words[0]?.toLowerCase();\n  if (firstWord !== expectedToken.toLowerCase()) {\n    loraTokenValid = false;\n    loraTokenIssue = `Refinement lost LoRA token! Expected \"${expectedToken}\", got \"${words[0]}\"`;\n  }\n}\n\nconst checks = {\n  hasMinLength: wordCount >= 10,\n  hasMaxLength: wordCount <= 200,\n  hasSubject: /\\b(person|character|animal|object)\\b/i.test(refined),\n  hasVisualDetails: /\\b(color|light|style|composition|texture|material)\\b/i.test(refined),\n  noErrors: !/\\b(error|unable)\\b/i.test(refined),\n  notTooVague: !/^(This is|The image)/.test(refined),\n  hasLighting: /\\b(light|shadow|bright|dark)\\b/i.test(refined),\n  hasComposition: /\\b(foreground|background|center|angle)\\b/i.test(refined),\n  loraTokenValid: expectedToken ? loraTokenValid : true\n};\n\nconst newScore = Object.values(checks).filter(Boolean).length / Object.keys(checks).length;\n\nreturn {\n  image_id: original.image_id,\n  filename: original.filename,\n  caption: refined,\n  word_count: wordCount,\n  quality_score: Math.round(newScore * 100) / 100,\n  quality_checks: checks,\n  was_refined: true,\n  original_quality: original.quality_score,\n  improvement: Math.round((newScore - original.quality_score) * 100) / 100,\n  success: true,\n  lora_token: expectedToken,\n  lora_token_valid: loraTokenValid,\n  lora_token_issue: loraTokenIssue\n};"
      },
      "id": "rescore",
      "name": "Rescore",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2220, 288]
    },
    {
      "parameters": {
        "jsCode": "// Aggregate all results\nconst allItems = $input.all();\nconst results = allItems.map(item => item.json);\n\nconst total = results.length;\nconst successful = results.filter(r => r.success).length;\nconst failed = results.filter(r => !r.success).length;\nconst refined = results.filter(r => r.was_refined).length;\n\nconst successfulResults = results.filter(r => r.success && r.caption);\nconst avgScore = successfulResults.length > 0\n  ? successfulResults.reduce((s, r) => s + r.quality_score, 0) / successfulResults.length\n  : 0;\nconst avgWords = successfulResults.length > 0\n  ? successfulResults.reduce((s, r) => s + r.word_count, 0) / successfulResults.length\n  : 0;\n\n// LoRA token summary\nconst loraResults = results.filter(r => r.lora_token);\nconst loraTokensValid = loraResults.filter(r => r.lora_token_valid).length;\nconst loraTokensInvalid = loraResults.filter(r => !r.lora_token_valid).length;\nconst loraIssues = results\n  .filter(r => r.lora_token_issue)\n  .map(r => ({ image_id: r.image_id, issue: r.lora_token_issue }));\n\nconst formatted = results.map(r => ({\n  image_id: r.image_id,\n  filename: r.filename,\n  caption: r.caption,\n  success: r.success,\n  word_count: r.word_count,\n  quality_score: r.quality_score,\n  was_refined: r.was_refined || false,\n  improvement: r.improvement || 0,\n  error: r.error,\n  lora_token: r.lora_token || null,\n  lora_token_valid: r.lora_token_valid || false,\n  lora_token_issue: r.lora_token_issue || null\n}));\n\nconst jobId = $node['Setup'].json.job_id;\nconst startedAt = $node['Setup'].json.started_at;\nconst completedAt = new Date().toISOString();\n\nreturn {\n  job_id: jobId,\n  status: 'completed',\n  processing_time: {\n    started_at: startedAt,\n    completed_at: completedAt,\n    duration_seconds: Math.round((new Date(completedAt) - new Date(startedAt)) / 1000)\n  },\n  results: formatted,\n  summary: {\n    total_images: total,\n    successful: successful,\n    failed: failed,\n    refined_count: refined,\n    avg_quality_score: Math.round(avgScore * 100) / 100,\n    avg_word_count: Math.round(avgWords),\n    lora_tokens_expected: loraResults.length,\n    lora_tokens_valid: loraTokensValid,\n    lora_tokens_invalid: loraTokensInvalid,\n    lora_issues: loraIssues\n  },\n  metadata: {\n    workflow_version: 'v3.0',\n    model: 'gpt-4o',\n    config: $node['Setup'].json.config\n  }\n};"
      },
      "id": "aggregate",
      "name": "Aggregate Results",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2440, 400]
    },
    {
      "parameters": {
        "respondWith": "allIncomingItems",
        "options": {}
      },
      "id": "respond",
      "name": "Respond",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [2660, 400]
    }
  ],
  "connections": {
    "Webhook": {
      "main": [[{"node": "Setup", "type": "main", "index": 0}]]
    },
    "Setup": {
      "main": [[{"node": "Split Out", "type": "main", "index": 0}]]
    },
    "Split Out": {
      "main": [[{"node": "Caption", "type": "main", "index": 0}]]
    },
    "Caption": {
      "main": [[{"node": "Check Subject Removal", "type": "main", "index": 0}]]
    },
    "Check Subject Removal": {
      "main": [
        [{"node": "Remove Subject Details", "type": "main", "index": 0}],
        [{"node": "Score Quality", "type": "main", "index": 0}]
      ]
    },
    "Remove Subject Details": {
      "main": [[{"node": "Score Quality", "type": "main", "index": 0}]]
    },
    "Score Quality": {
      "main": [[{"node": "Check Refinement", "type": "main", "index": 0}]]
    },
    "Check Refinement": {
      "main": [
        [{"node": "Refine Caption", "type": "main", "index": 0}],
        [{"node": "Aggregate Results", "type": "main", "index": 0}]
      ]
    },
    "Refine Caption": {
      "main": [[{"node": "Rescore", "type": "main", "index": 0}]]
    },
    "Rescore": {
      "main": [[{"node": "Aggregate Results", "type": "main", "index": 0}]]
    },
    "Aggregate Results": {
      "main": [[{"node": "Respond", "type": "main", "index": 0}]]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  }
}